「这里记录的是吴恩达Andrew Ng在[深度学习课程](http://mooc.study.163.com/learn/2001281003?tid=2001391036#/learn/content?type=detail&id=2001701052)中提到过的优化算法，以及其他受推荐的优化算法。最后日常感谢Andrew Ng的视频！」


## 梯度下降的优化

#### 1.指数加权平均

+ 引入概念

    在讲下面几个优化算法之前，先引出指数加权平均的概念。指数加权平均是一种简称，概念引自统计学中的指数加权移动平均EWMA(Exponentially Weighted Moving Average)。

![](http://upload-images.jianshu.io/upload_images/2759738-e625b7a8b0d9c5e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

   上面这张图是伦敦某一年的气温图，这张图看起来很凌乱，如果要计算趋势，也就是气温的局部平均值，或者说移动平均值，就需要这样做：先设定v<sub>0</sub>=0，然后在后一天的计算中，v<sub>0</sub> 乘以0.9的加权数再加上(1-0.9)乘以当天温度θ<sub>1</sub>，即：v<sub>1</sub>=0.9v<sub>0</sub>+(1-0.9)θ<sub>1</sub>,同理v<sub>2</sub>=0.9v<sub>1</sub>+(1-0.9)θ<sub>2</sub>, v<sub>3</sub>=0.9v<sub>2</sub>+(1-0.9)θ<sub>3</sub>,......,v<sub>t</sub>=0.9v<sub>t-1</sub>+(1-0.9)θ<sub>t</sub>，如此计算，用红线作图的话就能得到下图结果，这样就得到了气温的趋势，即移动平均值。

![](http://upload-images.jianshu.io/upload_images/2759738-c9c96628e683459c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

   引入一个参数β，计算公式可以表示为：**v<sub>t</sub> = βv<sub>t-1</sub>+(1-β)θ<sub>t</sub>**
当β=0.9的时候，得到的是平均过去1/(1-β)=10天的气温值，如下图红线所示；极端一点，当β=0.98时，平均的是过去1/(1-β)=50天的气温值，如下图绿色所示；再取另一个极端，当β=0.5时，平均的是过去1/(1-β)=2天的气温值，如下图黄色线所示。

![](http://upload-images.jianshu.io/upload_images/2759738-419d341ea64144c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

   从图中可以看出绿色的线比较平滑，因为它多平均了几天的气温，波动就会比较小；缺点就是曲线较之前发生了平移，这是因为给了前一天很高的加权值，而后一天的加权值就相对较小了。从图中可以看出黄色的线波动很大，这是因为它才平均了两天的气温值，存在很大的噪音。

+ 理解指数加权平均

     1）列举几个式子：
     v<sub>100</sub> = 0.9v<sub>99</sub> + 0.1θ<sub>100</sub>
     v<sub>99</sub> = 0.9v<sub>98</sub> + 0.1θ<sub>99</sub>
     v<sub>98</sub> = 0.9v<sub>97</sub> + 0.1θ<sub>98</sub>
        **...**
      2）层层带入后，得到下面的式子：
     v<sub>100</sub> = 0.1 θ<sub>100</sub> + 0.1×0.9 θ<sub>99</sub> + 0.1×0.9<sup>2</sup> θ<sub>98</sub> + 0.1×0.9<sup>3</sup> θ<sub>97</sub> + **...**
     这样的结果就相当于是拿每一天的气温数据和一个衰减的指数函数对应的每个元素相乘再相加的结果。其中每项气温数据前的系数是成指数衰减的，所以称为指数加权平均，且每项气温数据前的系数之和接近于1。之前的计算平均的气温天数的公式为1/(1-β)，是从β<sup>1/(1-β)</sup>≈1/e这个式子中得出的。也就是说，当β取0.9的时候，10天之后加权系数下降到最先系数的三分之一（1/e ,e≈2.7）

+ 指数加权平均的偏差修正

    在实际计算过程中，会得到如下图紫色的加权平均线，在曲线的初期阶段会和绿色的线存在偏差。可以用公式v<sub>t</sub> / (1 - β<sup>t</sup>)来修正。

![](http://upload-images.jianshu.io/upload_images/2759738-8874cb0b6d66b477.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 2.Momentum

Momentum，也称为Momentum梯度下降法，是一种快于标准梯度下降法的一种算法。Momentum所采用的方法就是应用上面提到的的指数加权平均来计算梯度的指数加权平均，并利用这个梯度来更新权重和偏置。
![](https://upload-images.jianshu.io/upload_images/2759738-53ded56773405105.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/854/format/webphttp://upload-images.jianshu.io/upload_images/2759738-53ded56773405105.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在上面这张图中，椭圆形状代表损失函数，中间的红点表示最小值，标准的梯度下降（如蓝线所示），一步步更新上下摆动到接近于最小值，正是因为这样的上下摆动拖慢了到达最小值附近的速度，也防止我们应用较大的学习速率。如果采用较大的学习速率，摆动就会超出函数的范围（如紫色线所示）。所以要采取的策略就是使垂直方向上的学习速度减慢，使水平方向上的学习速度加快。Momentum就是这样的一种策略。

具体做法是：
  - 初始化v<sub>dw</sub>=0，v<sub>db</sub>=0(维数与w，b保持一致)

    在t次迭代中:
    - 计算dw，db；
    - 然后应用上面的指数加权平均的公式计算**v<sub>dw</sub>=βv<sub>dw</sub>+(1-β)dw**,也就是dw的平均数;同理计算db的平均数**v<sub>db</sub>=βv<sub>db</sub>+(1-β)db**；
    - 最后更新权重**w = w-αv<sub>dw</sub>**和偏置**b = b-αv<sub>db</sub>** ，这里不需要使用偏差修正。

采用Momentum能够使垂直方向上的学习变慢的原因是计算上下摆动的平均值，平均过程中垂直方向上正负值抵消，最后可以使平均值接近于0。能够说明水平方向上加快是因为计算的所有的微分都指向水平方向的右侧，结果计算的横轴的平均值依然很大。最后在几次迭代后就能够快速的到达最小值附近（如上图红线所示）

#### 3.RMSprop
还有一个算法叫做RMSprop，全称是root mean square prop，也是一种能加快梯度下降的算法。策略也是在垂直方向上减慢学习速度，在水平方向上则加快学习速度。实行过程与Momentum相似，但有一点差异。

具体做法是：

+ 初始化s<sub>dw</sub>=0，s<sub>db</sub>=0(维数与w，b保持一致)

   在t次迭代中:在t次迭代中：
  - 计算dw，db；
  - 应用上面的指数加权平均的公式计算**s<sub>dw</sub>=βs<sub>dw</sub>+(1-β)dw<sup>2</sup>**;同理计算**s<sub>db</sub>=βs<sub>db</sub>+(1-β)db<sup>2</sup>**；
  - 最后更新权重**w = w-αdw/sqrt(s<sub>dw</sub>+ε)**和偏置**b = b-αdb/sqrt(s<sub>db</sub>+ε)**
  (常用ε=10<sup>-8</sup>,加上ε是防止分母为零的情况出现)



#### 4.Adam
Adam基本上就是结合了Momentum和RMSprop这两种算法。全称是Adaptive Moment Estimation。

具体做法是：
+ 初始化v<sub>dw</sub>=0，v<sub>db</sub>=0，s<sub>dw</sub>=0，s<sub>db</sub>=0(维数与w，b保持一致)
  在t次迭代中:
  - 计算dw，db；
  - 计算**v<sub>dw</sub>=β<sub>1</sub>v<sub>dw</sub>+(1-β<sub>1</sub>)dw**和**v<sub>db</sub>=β<sub>1</sub>v<sub>db</sub>+(1-β<sub>1</sub>)db**；
  - 计算**s<sub>dw</sub>=β<sub>2</sub>s<sub>dw</sub>+(1-β<sub>2</sub>)dw<sup>2</sup>**和**s<sub>db</sub>=β<sub>2</sub>s<sub>db</sub>+(1-β<sub>2</sub>)db<sup>2</sup>**；
  - 对v<sub>dw</sub>，v<sub>db</sub>进行偏差修正：
     **v<sub>dw</sub><sup>corrected</sup>=v<sub>dw</sub>/(1-β<sub>1</sub>)**，**v<sub>db</sub><sup>corrected</sup>=v<sub>db</sub>/(1-β<sub>1</sub>)**
  - 对s<sub>dw</sub>，s<sub>db</sub>进行偏差修正：
     **s<sub>dw</sub><sup>corrected</sup>=v<sub>dw</sub>/(1-β<sub>2</sub>)**，**s<sub>db</sub><sup>corrected</sup>=s<sub>db</sub>/(1-β<sub>2</sub>)**

  - 最后更新权重和偏置：
    **w = w-αdw/sqrt(s<sub>dw</sub><sup>corrected</sup>+ε)**，**b = b-αdb/sqrt(s<sub>db</sub><sup>corrected</sup>+ε)**
    (常用ε=10<sup>-8</sup>,加上ε是防止分母为零的情况出现)

对于超参数的选择：
- α：学习速率一直很重要，需要不断地调试
- β<sub>1</sub>：一般使用0.9，当做缺省值使用
- β<sub>1</sub>：Adam论文的作者推荐使用0.999，当做缺省值使用
- ε：不是特别重要的参数，并不会影响算法的结果，也不用去调试它，Adam论文的作者建议使用10<sup>-8</sup>。
