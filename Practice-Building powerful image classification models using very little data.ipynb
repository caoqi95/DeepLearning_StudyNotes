{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data pre-processing and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rotation_range:在（0,180）范围内取值，限制图片旋转的范围\n",
    "- width_shift and height_shift:取整个高和宽一部分的浮点数数\n",
    "- rescale: 在其他任何处理之前设定的一个值，原始图片的RGB参数在0-255之间，但这个数值对于一个典型的learning rate来说还是过大，所以把原始数据乘以rescale，使范围限制在（0,1）\n",
    "- shear-range:一个浮点数用于设定随意地应用剪切变换\n",
    "- zoom_range:一个浮点数用于设定随意地放大图片\n",
    "- horizontal_flip:True/False,用于设定是否水平翻转一半的图片\n",
    "- fill_mode:‘constant’，‘nearest’，‘reflect’或‘wrap’之一，用于填充新创建的像素的策略，其可以在旋转或宽度/高度偏移之后出现\n",
    "\n",
    "更多参数详见：http://keras-cn.readthedocs.io/en/latest/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array,load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "\n",
    "img = load_img('kaggle/train/cat/cat.0.jpg') \n",
    "x = img_to_array(img) # this is a Numpy array with shape (3,150,150)\n",
    "x = x.reshape((1,) + x.shape) # this is a Numpy array with shape (1,3,150,150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to be the 'preview/' directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x,batch_size=1,save_to_dir='preview',save_prefix='cat',save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i >20:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a small convnet from scratch:in 40 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 123s 980ms/step - loss: 0.7167 - acc: 0.5250 - val_loss: 0.6821 - val_acc: 0.6262\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 122s 973ms/step - loss: 0.6884 - acc: 0.5865 - val_loss: 0.6491 - val_acc: 0.6212\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 120s 957ms/step - loss: 0.6531 - acc: 0.6285 - val_loss: 0.7816 - val_acc: 0.5663\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 119s 956ms/step - loss: 0.6374 - acc: 0.6500 - val_loss: 0.9216 - val_acc: 0.5837\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 120s 956ms/step - loss: 0.6262 - acc: 0.6740 - val_loss: 0.5937 - val_acc: 0.6825\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 123s 981ms/step - loss: 0.5993 - acc: 0.6745 - val_loss: 0.6015 - val_acc: 0.6500\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 119s 949ms/step - loss: 0.6095 - acc: 0.6820 - val_loss: 0.5661 - val_acc: 0.7075\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 116s 932ms/step - loss: 0.5954 - acc: 0.6960 - val_loss: 0.5873 - val_acc: 0.6763\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 117s 933ms/step - loss: 0.5768 - acc: 0.7065 - val_loss: 0.5629 - val_acc: 0.7113\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 120s 956ms/step - loss: 0.5810 - acc: 0.7025 - val_loss: 0.6319 - val_acc: 0.6987\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 120s 963ms/step - loss: 0.5667 - acc: 0.7190 - val_loss: 0.6022 - val_acc: 0.6925\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 123s 983ms/step - loss: 0.5826 - acc: 0.7040 - val_loss: 0.6674 - val_acc: 0.7150\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 119s 950ms/step - loss: 0.5812 - acc: 0.7100 - val_loss: 0.8338 - val_acc: 0.6637\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 120s 962ms/step - loss: 0.6088 - acc: 0.7040 - val_loss: 0.5679 - val_acc: 0.7037\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 123s 983ms/step - loss: 0.5885 - acc: 0.7155 - val_loss: 0.6183 - val_acc: 0.6375\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 123s 982ms/step - loss: 0.5831 - acc: 0.7060 - val_loss: 0.5597 - val_acc: 0.7238\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 130s 1s/step - loss: 0.5624 - acc: 0.7235 - val_loss: 0.6169 - val_acc: 0.6887\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 129s 1s/step - loss: 0.5547 - acc: 0.7370 - val_loss: 0.5641 - val_acc: 0.7175\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 124s 991ms/step - loss: 0.5487 - acc: 0.7340 - val_loss: 0.5698 - val_acc: 0.7338\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 124s 991ms/step - loss: 0.5637 - acc: 0.7325 - val_loss: 0.5703 - val_acc: 0.7000\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 122s 977ms/step - loss: 0.5339 - acc: 0.7360 - val_loss: 0.5617 - val_acc: 0.7075\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 120s 961ms/step - loss: 0.5263 - acc: 0.7470 - val_loss: 0.5702 - val_acc: 0.7150\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 123s 985ms/step - loss: 0.5692 - acc: 0.7275 - val_loss: 0.5948 - val_acc: 0.6750\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 118s 945ms/step - loss: 0.5591 - acc: 0.7350 - val_loss: 0.6145 - val_acc: 0.6950\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 118s 945ms/step - loss: 0.5384 - acc: 0.7445 - val_loss: 0.5764 - val_acc: 0.7050\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 116s 931ms/step - loss: 0.5371 - acc: 0.7390 - val_loss: 0.5898 - val_acc: 0.7388\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 121s 971ms/step - loss: 0.5632 - acc: 0.7500 - val_loss: 0.5896 - val_acc: 0.7238\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 120s 961ms/step - loss: 0.5500 - acc: 0.7400 - val_loss: 0.5551 - val_acc: 0.7375\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 120s 961ms/step - loss: 0.5355 - acc: 0.7590 - val_loss: 0.6157 - val_acc: 0.6987\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 120s 959ms/step - loss: 0.5407 - acc: 0.7575 - val_loss: 0.5415 - val_acc: 0.7475\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 122s 973ms/step - loss: 0.5486 - acc: 0.7600 - val_loss: 0.5392 - val_acc: 0.7362\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 119s 950ms/step - loss: 0.5277 - acc: 0.7520 - val_loss: 0.5744 - val_acc: 0.7175\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 118s 946ms/step - loss: 0.4968 - acc: 0.7750 - val_loss: 0.5601 - val_acc: 0.7250\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 118s 941ms/step - loss: 0.5201 - acc: 0.7600 - val_loss: 0.5450 - val_acc: 0.7350\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 118s 943ms/step - loss: 0.5126 - acc: 0.7735 - val_loss: 0.6385 - val_acc: 0.6913\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 118s 941ms/step - loss: 0.5125 - acc: 0.7575 - val_loss: 0.7262 - val_acc: 0.7212\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 118s 940ms/step - loss: 0.5088 - acc: 0.7690 - val_loss: 0.6104 - val_acc: 0.6887\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 120s 959ms/step - loss: 0.5052 - acc: 0.7650 - val_loss: 0.5625 - val_acc: 0.7075\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 120s 964ms/step - loss: 0.4934 - acc: 0.7695 - val_loss: 0.5523 - val_acc: 0.7225\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 121s 970ms/step - loss: 0.5236 - acc: 0.7700 - val_loss: 0.5385 - val_acc: 0.7288\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 128s 1s/step - loss: 0.4828 - acc: 0.7760 - val_loss: 0.7823 - val_acc: 0.7087\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 122s 979ms/step - loss: 0.4918 - acc: 0.7750 - val_loss: 0.5727 - val_acc: 0.7212\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 117s 937ms/step - loss: 0.5042 - acc: 0.7640 - val_loss: 0.5444 - val_acc: 0.7338\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 117s 938ms/step - loss: 0.5012 - acc: 0.7555 - val_loss: 0.6391 - val_acc: 0.7075\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 117s 937ms/step - loss: 0.5293 - acc: 0.7520 - val_loss: 0.5505 - val_acc: 0.7425\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 117s 936ms/step - loss: 0.5148 - acc: 0.7640 - val_loss: 0.5595 - val_acc: 0.7450\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 118s 942ms/step - loss: 0.5177 - acc: 0.7675 - val_loss: 0.5808 - val_acc: 0.6637\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 117s 938ms/step - loss: 0.5101 - acc: 0.7870 - val_loss: 0.5668 - val_acc: 0.7125\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 119s 950ms/step - loss: 0.4886 - acc: 0.7760 - val_loss: 0.5582 - val_acc: 0.7300\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 124s 993ms/step - loss: 0.5138 - acc: 0.7560 - val_loss: 0.5635 - val_acc: 0.7312\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.layers import Activation,Dropout,Flatten,Dense\n",
    "from keras import backend as K \n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "\n",
    "train_data_dir = 'kaggle/train'\n",
    "validation_data_dir = 'kaggle/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "                        rescale=1./255,\n",
    "                        shear_range=0.2,\n",
    "                        zoom_range=0.2,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in subfolers of 'kaggle/train',and indefinitely generate\n",
    "# batches of augmented image data \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "              train_data_dir, \n",
    "              target_size=(img_width, img_height),\n",
    "              batch_size=batch_size,\n",
    "              class_mode='binary') \n",
    "\n",
    "\n",
    "# this is a similar generator,for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)\n",
    "model.save_weights('first_try.h5')  # always save weights after training or during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the bottleneck features of a pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用VGG的原因：\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset --a model previously featured on this blog. Because the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs. cats classification problem extremely well. However, the method we present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 0.6613 - acc: 0.7545 - val_loss: 0.3028 - val_acc: 0.8725\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.3658 - acc: 0.8535 - val_loss: 0.2511 - val_acc: 0.9012\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.3097 - acc: 0.8865 - val_loss: 0.2360 - val_acc: 0.9038\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.2544 - acc: 0.9050 - val_loss: 0.2791 - val_acc: 0.9087\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.2533 - acc: 0.9065 - val_loss: 0.2421 - val_acc: 0.8962\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.2374 - acc: 0.9120 - val_loss: 0.3021 - val_acc: 0.9100\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1701 - acc: 0.9395 - val_loss: 0.3327 - val_acc: 0.8925\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1712 - acc: 0.9415 - val_loss: 0.3462 - val_acc: 0.8912\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 0.1443 - acc: 0.9480 - val_loss: 0.3363 - val_acc: 0.9050\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1231 - acc: 0.9510 - val_loss: 0.3773 - val_acc: 0.8912\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1138 - acc: 0.9575 - val_loss: 0.4262 - val_acc: 0.8912\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0880 - acc: 0.9675 - val_loss: 0.4804 - val_acc: 0.8912\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0960 - acc: 0.9650 - val_loss: 0.4359 - val_acc: 0.8925\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0903 - acc: 0.9675 - val_loss: 0.5791 - val_acc: 0.8775\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0747 - acc: 0.9740 - val_loss: 0.6928 - val_acc: 0.8800\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0587 - acc: 0.9790 - val_loss: 0.5341 - val_acc: 0.9025\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0679 - acc: 0.9785 - val_loss: 0.7830 - val_acc: 0.8775\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0827 - acc: 0.9750 - val_loss: 0.4845 - val_acc: 0.9113\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0499 - acc: 0.9825 - val_loss: 0.5968 - val_acc: 0.8938\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0489 - acc: 0.9800 - val_loss: 0.5790 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0349 - acc: 0.9845 - val_loss: 0.6066 - val_acc: 0.9087\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0428 - acc: 0.9870 - val_loss: 0.8211 - val_acc: 0.8850\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0291 - acc: 0.9880 - val_loss: 0.9032 - val_acc: 0.8788\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.7031 - val_acc: 0.9012\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0335 - acc: 0.9900 - val_loss: 0.7155 - val_acc: 0.9038\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0224 - acc: 0.9905 - val_loss: 0.7380 - val_acc: 0.8938\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0273 - acc: 0.9930 - val_loss: 0.7909 - val_acc: 0.9025\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0277 - acc: 0.9915 - val_loss: 0.7920 - val_acc: 0.8950\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0276 - acc: 0.9915 - val_loss: 0.8216 - val_acc: 0.8938\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0254 - acc: 0.9920 - val_loss: 0.8028 - val_acc: 0.8875\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0231 - acc: 0.9915 - val_loss: 0.8220 - val_acc: 0.9050\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 0.0270 - acc: 0.9910 - val_loss: 0.8379 - val_acc: 0.8950\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 0.0286 - acc: 0.9930 - val_loss: 0.8494 - val_acc: 0.8938\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0185 - acc: 0.9945 - val_loss: 0.8058 - val_acc: 0.8962\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0248 - acc: 0.9920 - val_loss: 0.9873 - val_acc: 0.8788\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0178 - acc: 0.9950 - val_loss: 0.8310 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.9309 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0408 - acc: 0.9905 - val_loss: 0.9732 - val_acc: 0.8950\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0231 - acc: 0.9940 - val_loss: 1.0021 - val_acc: 0.8875\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0175 - acc: 0.9955 - val_loss: 0.8630 - val_acc: 0.8938\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0249 - acc: 0.9930 - val_loss: 0.9208 - val_acc: 0.9025\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0068 - acc: 0.9975 - val_loss: 1.0490 - val_acc: 0.8800\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.9972 - val_acc: 0.8988\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 0.0231 - acc: 0.9935 - val_loss: 0.9252 - val_acc: 0.8950\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0218 - acc: 0.9935 - val_loss: 0.9005 - val_acc: 0.9025\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0085 - acc: 0.9975 - val_loss: 1.0176 - val_acc: 0.8900\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0280 - acc: 0.9935 - val_loss: 0.8789 - val_acc: 0.8975\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.9285 - val_acc: 0.8975\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0104 - acc: 0.9955 - val_loss: 1.0309 - val_acc: 0.8925\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.0068 - acc: 0.9985 - val_loss: 1.0049 - val_acc: 0.8925\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'kaggle/train'\n",
    "validation_data_dir = 'kaggle/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50 \n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "# save bottle features\n",
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    \n",
    "    # bulid the VGG network\n",
    "    model = applications.VGG16(include_top=False, weights = 'imagenet')\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "                train_data_dir,\n",
    "                target_size=(img_width, img_height),\n",
    "                batch_size=batch_size,\n",
    "                class_mode=None,\n",
    "                shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "                bottleneck_features_train)\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "                validation_data_dir,\n",
    "                target_size=(img_width, img_height),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False) # be in order \n",
    "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
    "                bottleneck_features_validation)\n",
    "    \n",
    "    \n",
    "    \n",
    "# train the model \n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.array(\n",
    "            [0] * int(nb_train_samples / 2) + [1] *int(nb_train_samples /2))  # set the cat label is 0, the dog labels is 1\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "    validation_labels = np.array(\n",
    "            [0] * int(nb_validation_samples / 2) + [1] * int(nb_validation_samples /2))\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                        loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "                 epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "\n",
    "save_bottleneck_features()\n",
    "train_top_model()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the top layers of a a pre-trained network\n",
    "\n",
    "Note that:\n",
    "\n",
    "- in order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.\n",
    "- we choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).\n",
    "- fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "(4, 4, 512)\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 1201s 10s/step - loss: 0.4728 - acc: 0.9385 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 1214s 10s/step - loss: 0.5400 - acc: 0.9310 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 1195s 10s/step - loss: 0.4216 - acc: 0.9360 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 1188s 10s/step - loss: 0.4660 - acc: 0.9325 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 1177s 9s/step - loss: 0.5041 - acc: 0.9330 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 1132s 9s/step - loss: 0.4785 - acc: 0.9350 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 1126s 9s/step - loss: 0.4958 - acc: 0.9330 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 1141s 9s/step - loss: 0.5204 - acc: 0.9305 - val_loss: 1.0049 - val_acc: 0.8925\n",
      "Epoch 9/50\n",
      " 17/125 [===>..........................] - ETA: 12:23 - loss: 0.9805 - acc: 0.9044"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'kaggle/train'\n",
    "validation_data_dir = 'kaggle/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# build the VGG16 network\n",
    "model = VGG16(weights='imagenet', include_top=False,input_shape=(img_width, img_height, 3))\n",
    "print('Model loaded.')\n",
    "print(model.output_shape[1:])\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "# model.add(top_model)\n",
    "model = Model(inputs=model.input, outputs=top_model(model.output))\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "none",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
